# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1027EBtaT947PohTNjf4LcfmnH18cSKVh
"""

import pandas as pd
import torch
import string
import sys 
from collections import defaultdict
from torch import nn

def p(thing):
    sys.stdout.write(thing)


def isnan(x):
    if type(x) == str:
        return False
    try:
        int(x)
        return False
    except:
        return True


class pbar:
    def __init__(self, length, total, frac=False):
        self.length = length
        self.total = total
        if not frac:
            p('|' + '.' * length + '|')
        else:
            pass
        self.count = 0
        self.thresh = total // length
        self.all_count = 0
        self.n = 0

    def frac(self):
        if self.n == self.total - 1:
            p('\n')
            return
        if self.n > 0:
            p('\b' * self.len)
        string = f'{self.n}/{self.total}'
        p(string)
        self.len = len(string)
        self.n += 1


class fbar:
    def __init__(self, length):
        self.length = length - 1
        self.count = 0
        self.slen = 0

    def step(self):
        if self.count == self.length:
            p('\b' * self.slen)
            print(f'{self.count}/{self.length}')
            return
        p('\b' * self.slen)
        s = f'{self.count}/{self.length} '
        self.slen = len(s)
        p(s)
        self.count += 1


newsdf = pd.read_csv('./corona_fake.csv')
titles = newsdf['title']
text = newsdf['text']
labels = newsdf['label']
letters = string.ascii_letters + ' !0123456789?'


def enum1(x):
    for i in range(len(x)):
        yield (i + 1, x[i])


vocab = {letter: i for i, letter in enum1(letters)}
vocab = defaultdict(lambda: 0, vocab)
#print(vocab[''],vocab['7'])


def word2tensor(word):
    tens = [vocab[i] for i in word]
    if len(tens)>0:
      return torch.tensor(tens)
    else:
      return torch.tensor([0])


def stack(data):
    result = []
    for i in range(len(data)):
        try:
            result.append(word2tensor(data[i]))
        except:
            result.append(torch.zeros(1))
    return result


tensortext = stack(text)
tensorlabel = []
for label in labels:
    if label == 'Fake' or label == 'fake':
        tensorlabel.append(torch.zeros(1))
    elif label == 'TRUE':
        tensorlabel.append(torch.ones(1))
    else:
        # print(label,'n')
        tensorlabel.append(-1)
tensortitles = stack(titles)


def greatest_len(x):
    great = len(x[0])
    for i in x:
        if len(i) > great:
            great = i
        return great


def pad_to(x, length):
  if len(x)>=length:
    return x
  return torch.nn.functional.pad(x, (0, length - len(x)))


def clean(thing, length):
    new = []
    for sentence in thing:
        new.append(pad_to(sentence, length))
    return new


titlemax = 150
textmax = 400
ptext = clean(tensortext, textmax)
ptitle = clean(tensortitles, titlemax)
len(ptitle), len(ptext), len(tensorlabel)
all_data = []


def batcher(x, y, batch):
    c = batch
    re = []
    for i in range(0, len(x), batch):
        re.append((x[i:c], y[i:c]))
    return re


for j in range(len(ptitle)):
    if tensorlabel[j] < 0:
        # print(tensorlabel[j])
        pass
    elif sum(ptext[j]).sum() == 0 and sum(ptitle[j]).sum() == 0:
        pass
    else:
        all_data.append((ptitle[j], ptext[j], tensorlabel[j]))
train_data = all_data[264:]
val_data = all_data[:264]

def list2tensor(l):
  out=torch.empty(len(l),*l[0].shape)
  return torch.cat(l,out=out)
# print('done')
import math
split = 100
val_titles = list(titles)[:split]
val_text = list(text)[:split]
train_titles = list(titles)[split:]
train_labels = tensorlabel[split:]
train_text = list(text)[split:]
vali_labels=list(tensorlabel)[:split]
all_text=[(text[i],tensorlabel[i]) for i in range(len(text)) if not isnan(text[i]) and tensorlabel[i]>-1]
pro_train_text,pro_val_text=torch.utils.data.random_split(all_text,[len(all_text)-264,264])
with open('./stopwords.txt') as st:
  for i in st:
    stopwords=eval(i)
def remove_stops(x,tolist=False):
  if tolist:
    return list(filter(lambda x: x not in stopwords,x))
  else:
    return filter(lambda x: x not in stopwords,x)
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def train_better(ob,trains,vals,epochs,optim='sgd',lr=0.01,loss='bce'):
  try:
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model=ob().to(device)
    if loss=='bce':
      lossf=nn.BCELoss()
      fil_func=lambda x: x
    elif loss=='logs':
      lossf=nn.BCEWithLogitsLoss()
      fil_func=lambda x:torch.sigmoid(x)
    else:
      lossf=nn.MSELoss()
      fil_func=lambda x:x
    if optim=='sgd':
      o=torch.optim.SGD(model.parameters(),lr)
    elif optim=='adam':
      o=torch.optim.Adam(model.parameters(),lr)
    elif optim=='sadam':
      o=torch.optim.SparseAdam(model.parameters(),lr)
    losses=[float('inf'),float('inf'),float('inf')]
    valis=[float('inf'),float('inf'),float('inf')]
    for epoch in range(epochs):
      p=fbar(len(trains))
      for i,j in trains:
        p.step()
        pred=model(i)
        target=j.float().to(device)
        loss=lossf(pred.float(),target)
        model.zero_grad()
        o.zero_grad()
        loss.backward()
        o.step()
      model.eval()
      with torch.no_grad():
        count=0
        for vi,vt in vals:
          pred=model(vi)
          if torch.round(fil_func(pred))==vt.to(device):
              count+=1
      model.train()
      print(count/len(vals),f' epoch: {epoch}')
      print(loss)
      valis.append(count/len(vals))
      losses.append(loss)
      flat(losses,valis,o,lr)
    return model
  except KeyboardInterrupt:
    return model
def already_train_better(model,trains,vals,epochs,optim='sgd',lr=0.01,loss='bce',deflat=True,save=None):
  try:
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if loss=='bce':
      lossf=nn.BCELoss()
      fil_func=lambda x: x
    elif loss=='logs':
      lossf=nn.BCEWithLogitsLoss()
      fil_func=lambda x:torch.sigmoid(x)
    else:
      lossf=nn.MSELoss()
      fil_func=lambda x:x
    if optim=='sgd':
      o=torch.optim.SGD(model.parameters(),lr)
    elif optim=='adam':
      o=torch.optim.Adam(model.parameters(),lr)
    elif optim=='sadam':
      o=torch.optim.SparseAdam(model.parameters(),lr)
    losses=[float('inf'),float('inf'),float('inf')]
    valis=[float('inf'),float('inf'),float('inf')]
    for epoch in range(epochs):
      p=fbar(len(trains))
      for i,j in trains:
        p.step()
        pred=model(i)
        target=j.float().to(device)
        loss=lossf(pred.float(),target)
        model.zero_grad()
        o.zero_grad()
        loss.backward()
        o.step()
      model.eval()
      with torch.no_grad():
        count=0
        for vi,vt in vals:
          pred=model(vi)
          if torch.round(fil_func(pred))==vt.to(device):
              count+=1
      model.train()
      print(count/len(vals),f' epoch: {epoch}')
      print(loss)
      valis.append(count/len(vals))
      losses.append(loss)
      if save is not None:
        if count/len(vals)>save:
          torch.save(model,f'model{save}')
          files.download(f'model{save}')
      if deflat:
        flat(losses,valis,o,lr)
    return model
  except KeyboardInterrupt:
    return model
def within(x,y,s):
  return abs(x-y)<s
def mean_deviation(x):
  mean=sum(x)/len(x)
  means=[abs(i-mean) for i in x]
  return sum(means)/len(means)
def flat(x,x2,opt,olr):
  if mean_deviation(x[-3:])<0.002 or mean_deviation(x2[-3:])<0.002:
    for i in opt.param_groups:
      i['lr']+=0.00002
  else:
    if opt.param_groups[0]['lr']!=olr and not within(opt.param_groups[0]['lr'],olr,0.000001):
      for param in opt.param_groups:
        param['lr']-=0.00001
    if opt.param_groups[0]['lr']<olr:
      for pa in opt.param_groups:
        pa['lr']=olr

all_text=[(text[i],tensorlabel[i]) for i in range(len(text)) if not isnan(text[i]) and tensorlabel[i]>-1]
pro_train_text,pro_val_text=torch.utils.data.random_split(all_text,[len(all_text)-264,264])
def get_fake(x):
  f=0
  for i,j in x:
    if j==0:
      f+=1
  return f/len(x)
get_fake(pro_val_text),get_fake(pro_train_text)

get_fake(all_text[70:])

import torch
from torch import nn
class Thiccatten(nn.Module):
  def __init__(self,k,heads,mask=False):
    super().__init__()
    self.qw=nn.Linear(k,k*heads)
    self.kw=nn.Linear(k,k*heads)
    self.vw=nn.Linear(k,k*heads)
    self.fc=nn.Linear(k*heads,k)
    self.heads=heads
    self.mask=mask
  def forward(self,x):
    b,t,k=x.size()
    h=self.heads
    q=self.qw(x).view(b,t,h,k)
    key=self.kw(x).view(b,t,h,k)
    v=self.vw(x).view(b,t,h,k)
    keys = key.transpose(1, 2).contiguous().view(b * h, t, k)
    queries = q.transpose(1, 2).contiguous().view(b * h, t, k)
    values = v.transpose(1, 2).contiguous().view(b * h, t, k)
    keys=keys/(k**0.25)
    queries=queries/(k**0.25)
    dot=torch.bmm(keys,queries.transpose(1,2))#shape b*h,t,t
    if self.mask:
      it=torch.triu_indices(t,t,offset=1)
      dot[:,it[0],it[1]]=float('-inf')
    scaled_dot=torch.softmax(dot,dim=2)
    out = torch.bmm(scaled_dot, values).view(b, h, t, k)
    out=out.transpose(1,2).contiguous().view(b,t,k*h)
    return self.fc(out)

class tblock(nn.Module):
  def __init__(self, k, heads, mask=False):
    super().__init__()

    self.attention = Thiccatten(k, heads=heads,mask=mask)

    self.norm1 = nn.LayerNorm(k)
    self.norm2 = nn.LayerNorm(k)

    self.ff = nn.Sequential(
      nn.Linear(k, 4 * k),
      nn.ReLU(),
      nn.Linear(4 * k, k))

  def forward(self, x):
    attended = self.attention(x)
    x = self.norm1(attended + x)
    
    fedforward = self.ff(x)
    return self.norm2(fedforward + x)

class c_transformer(nn.Module):
  def __init__(self,heads=8,depth=7,word_embed=20,max_seq=6000,mode='mean'):
    super().__init__()
    self.transformers=nn.Sequential(*[tblock(word_embed,heads) for i in range(depth)])
    self.w_embed=nn.EmbeddingBag(len(vocab)+1,word_embed,mode=mode)
    self.pos_embed=nn.Embedding(max_seq+1,word_embed)
    self.fc=nn.Linear(word_embed,1)
  def forward(self,x):
    w=torch.stack([self.w_embed(word2tensor(i).unsqueeze(0)) for i in remove_stops(x.split(' '))]).transpose(0,1).to(device)
    b,t,k=w.size()
    try:
      pos_embeddings=self.pos_embed(torch.arange(t)).expand(b,t,k)
    except:
      print(w.size(),x)
    attended=self.transformers(pos_embeddings+w)
    classes=self.fc(attended).mean(dim=1)
    return torch.sigmoid(classes.reshape(-1))

class modified_transformer(nn.Module):
  def __init__(self,heads=8,depth=7,word_embed=20,max_seq=6000,out=1,mode='mean'):
    super().__init__()
    self.transformers=nn.Sequential(*[tblock(word_embed,heads) for i in range(depth)])
    self.w_embed=nn.EmbeddingBag(len(vocab)+1,word_embed,mode=mode)
    self.pos_embed=nn.Embedding(max_seq+1,word_embed)
    self.fc=nn.Linear(word_embed*2,out)

  def forward(self,x):
    w=torch.stack([self.w_embed(word2tensor(i).unsqueeze(0)) for i in remove_stops(x.split(' '))]).transpose(0,1).to(device)
    b,t,k=w.size()
    pos_embeddings=self.pos_embed(torch.arange(t)).expand(b,t,k)
    attended=self.transformers(pos_embeddings+w)
    classes=self.fc(torch.cat((attended,w+pos_embeddings),dim=-1)).mean(dim=1)
    return torch.sigmoid(classes.reshape(-1))
class whole(nn.Module):
  def __init__(self,heads=8,depth=7,word_embed=20,max_seq=6000,mode='mean',out=1):
    super().__init__()
    self.transformers=nn.Sequential(*[tblock(word_embed,heads) for i in range(depth)])
    self.w_embed=nn.EmbeddingBag(len(vocab)+1,word_embed,mode=mode)
    self.pos_embed=nn.Embedding(max_seq+1,word_embed)
    self.fc=nn.Linear(word_embed,out)
  def forward(self,x):
    w=torch.stack([self.w_embed(word2tensor(i).unsqueeze(0)) for i in remove_stops(x.split(' '))]).transpose(0,1).to(device)
    b,t,k=w.size()
    try:
      pos_embeddings=self.pos_embed(torch.arange(t)).expand(b,t,k)
    except:
      print(w.size(),x)
    attended=self.transformers(pos_embeddings+w)
    classes=self.fc(attended).mean(dim=1)
    return torch.sigmoid(classes.reshape(-1))
class dumb_gen(nn.Module):
  def __init__(self,heads=8,depth=7,embed_dim=1,max_seq=6000,out=1,mask=True):
    super().__init__()
    self.transformers=nn.Sequential(*[tblock(embed_dim,heads,mask=mask) for i in range(depth)])
    self.pos_embed=nn.Embedding(max_seq+1,embed_dim)
    self.fc=nn.Linear(embed_dim,out)
  def forward(self,x):
    b,t,k=x.size()
    pos_embeddings=self.pos_embed(torch.arange(t)).expand(b,t,k)
    attended=self.transformers(pos_embeddings+x)
    classes=self.fc(attended)
    return classes

#m=modified_transformer(mode='sum')
#msum=already_train_better(m,pro_train_text,pro_val_text,30,deflat=False)

torch.save(titleh,'title_model85')
files.download('title_model85')

new_sum_inator=c_transformer(mode='sum')
new_sum_inator.load_state_dict(hsum.state_dict())
hsum2=already_train_better(new_sum_inator,pro_train_text,pro_val_text,30,deflat=False)

from google.colab import files
title_model=c_transformer(mode='sum')
pro_train_titles,pro_val_titles=torch.utils.data.random_split(all_titles,[len(all_titles)-400,400])
print(get_fake(pro_val_titles))
titleh2=already_train_better(title_model,pro_train_titles,pro_val_titles,25,optim='sgd')

hlast2=already_train_better(hlast,pro_train_text,pro_val_text,30,deflat=False,save=.81)

m_try=modified_transformer(mode='sum')
hlast=already_train_better(m_try,pro_train_text,pro_val_text,30,deflat=False,save=.81)

best=torch.load('/content/model80')
best.eval()
c=0

all_titles=[(titles[i],tensorlabel[i]) for i in range(len(titles)) if not isnan(titles[i]) and tensorlabel[i]>-1]
fb=fbar(len(all_text))
def validate(m,data):
  fb=fbar(len(data))
  m.eval()
  c=0
  with torch.no_grad():
    for i,j in data:
      if torch.round(m(i))==j:
        c+=1
      fb.step()
    print(c/len(data))
    m.train()
validate(best,all_text)

g=0
for i in pro_train_text:
  if i[0]==text[7]:
    print(i)
text[7]

import time
def csv_writer(x,fname,firsts):
  with open(fname,'w') as c:
    c.write(firsts+'\n')
    for i,j in x:
      y=i.replace('\n','')
      c.write(f"{y}|{int(j)}\n")
def csv_reader(fname):
  with open(fname,'r') as r:
    d=list(r)k,l p
    heads=[[] for i in d[0].split('|')]
    for line in d[1:]:
      for i in range(len(heads)):
        heads[i].append(line.split('|')[i])
    return {d[0].split('|')[i].strip('\n'):heads[i] for i in range(len(heads))}
csv_writer(all_text[:10],'test_write.csv','text|labels')
csv_reader('good_training.txt')

with open('good_training.txt') as df:
  d=list(df)
  print(d[827])

def mnist_averager(data):
  averages=[[] for i in range(10)]
  for i in data:
    averages[int(i[0])].append(i[1:])
  for digit in averages:
    digit=sum(digit)/len(digit)
  return averages
def greatest(x):
  great=x[0]
  for i in x:
    if i>great:
      great=i
    return great
def least_diff(x,averages):
  diffs=[]
  for i in averages:
    diffs.append(sum(abs(x-i)))
  return greatest(diffs),diffs.index(greatest(diffs))
import numpy as np
mnist=np.loadtxt('sample_data/mnist_train_small.csv',delimiter=',')
d=[]
l=[]
def one_hot(x):
  mask=[0 for i in range(10)]
  mask[int(x)]=1
for i in mnist:
  l.append(one_hot(i[0]))
  d.append(i[1:])

class naive:
  def __init__(self,data):
    all=[[] for i in range(10)]
    for i in data:
      all[int(i[0])].append(i[1:])
    averages=[sum(i)/len(i) for i in all]
    self.averages=averages
  def predict(self,x):
    diffs=[]
    for i in self.averages:
      diffs.append(sum(abs(x-i)))
    return diffs.index(min(diffs))
n_pred=naive(mnist)

cor=0
for i in tqdm.tqdm(mnist):
  if i[0]==n_pred.predict(i[1:]):
    cor+=1

t=[]
l=[]
with open('good_training.txt') as df:
  d=list(df)
  heads=[[] for i in d[0].split('|')]
  for line in d[1:]:
    for i in range(len(heads)):
      heads[i].append(line.split('|')[i])
  stuff={d[0].split('|')[i].strip('\n'):heads[i] for i in range(len(heads))}

for i in stuff['text']:
  print(i)
  input()

# Commented out IPython magic to ensure Python compatibility.
import math
import torch
from torch import nn
sine=scaler([math.sin(i*math.pi/180)*180 for i in range(540)])
inputs=[]
labels=[]
for i in range(1,len(sine)-1):
  inputs.append(torch.FloatTensor(sine[:i]))
  labels.append(torch.FloatTensor([sine[1:i+1]]))
import matplotlib.pyplot as plt
# %matplotlib inline
plt.plot(sine)
plt.show()

import sys
from tqdm import tqdm
class fbar:
    def __init__(self, length):
        self.length = length - 1
        self.count = 0
        self.slen = 0

    def step(self):
        if self.count == self.length:
            sys.stdout.write('\b' * self.slen)
            print(f'{self.count}/{self.length}')
            return
        sys.stdout.write('\b' * self.slen)
        s = f'{self.count}/{self.length} '
        self.slen = len(s)
        sys.stdout.write(s)
        self.count += 1

seed=sine[:180]
sin_transformer=dumb_gen(depth=3,out=1,mask=True)
optim=torch.optim.Adam(sin_transformer.parameters(),lr=0.00001)
lossf=torch.nn.MSELoss()
start=sine[:180]
new_labels=labels[179:]
for epoch in range(30):
  plt.clf()
  start=sine[:180]
  new_labels=labels[179:]
  for i in range(len(new_labels)):
    pred=torch.sigmoid(sin_transformer(torch.FloatTensor(start).reshape(1,-1,1)))
    start.append(float(pred[0][-1].reshape(-1)))
    sin_transformer.zero_grad()
    optim.zero_grad()
    loss=lossf(pred.squeeze(0),new_labels[i].t())
    loss.backward()
    optim.step()
  sin_transformer.eval()
  with torch.no_grad():
    baseline=sine[:180]
    forced=sine[:180]
    for j in range(180):
      new_pred=sin_transformer(torch.FloatTensor(forced).reshape(1,-1,1))
      forced.append(sine[180+j])
      baseline.append(float(torch.sigmoid(new_pred[0][-1].reshape(-1))))
    plt.plot(baseline)
    plt.show()
  sin_transformer.train()
  print(loss)

def onehot(x,largest):
  h=[0 for i in range(largest)]
  h[int(x)]=1
  return h
def scaler(x):
  mini=min(x)
  maxi=max(x)
  new=[(i-mini)/(maxi-mini) for i in x]
  return new
def unscale(x,maximum,minimum):
  return [i*(maximum-minimum)+minimum for i in x]
t=torch.randn(2,3,4)
it=torch.triu_indices(3,4,offset=0)
t[:,it[0],it[1]]=0
print(t)

stoopid=dumb_gen(heads=1,depth=1)
stoopid(torch.randn(2,10,1))

new_labels[189].shape

import torch
lis=torch.nn.CrossEntropyLoss()
inp=torch.randn(1,10)
ans=torch.tensor([3])
lis(inp,ans)

